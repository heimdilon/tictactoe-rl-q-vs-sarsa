# Tic-Tac-Toe Q-Learning vs SARSA ğŸ®

Bu proje, klasik 3x3 Tic-Tac-Toe oyununda **Q-Learning** (off-policy) ve **SARSA** (on-policy) algoritmalarÄ±nÄ±n performansÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±r.

## ğŸ“‹ Ã–zellikler

- âœ… **Q-Learning vs SARSA** karÅŸÄ±laÅŸtÄ±rmasÄ± (off-policy vs on-policy)
- âœ… **Self-play**, **cross-play** ve **baseline** eÄŸitim stratejileri
- âœ… **Minimax** ve **Random** rakiplere karÅŸÄ± turnuva
- âœ… **Seaborn** ile modern gÃ¶rselleÅŸtirmeler
- âœ… **JSON/CSV** formatÄ±nda detaylÄ± Ã§Ä±ktÄ±lar
- âœ… **5478** geÃ§erli durum (filtered MDP space)
- âœ… **Heatmap** ile hÃ¼cre tercih analizi
- âœ… **Epsilon-greedy** keÅŸif mekanizmasÄ±
- âœ… **Hareketli ortalama** ile eÄŸitim trend analizi

## ğŸ“– Ä°Ã§indekiler

- [Kurulum](#kurulum)
- [KullanÄ±m](#kullanÄ±m)
- [Proje YapÄ±sÄ±](#proje-yapÄ±sÄ±)
- [Algoritmalar](#algoritmalar)
- [Ã‡Ä±ktÄ±lar](#Ã§Ä±ktÄ±lar)
- [SonuÃ§lar](#sonuÃ§lar)
- [KatkÄ±da Bulunma](#katkÄ±da-bulunma)
- [Lisans](#lisans)

## ğŸš€ Kurulum

### Gerekli BaÄŸÄ±mlÄ±lÄ±klar

```bash
pip install numpy matplotlib seaborn pandas
```

Veya `requirements.txt` kullanarak:

```bash
pip install -r requirements.txt
```

### Platform-Specific Kurulum

#### Windows
```cmd
# CMD
python -m pip install numpy matplotlib seaborn pandas

# PowerShell
pip install numpy matplotlib seaborn pandas
```

#### Linux / macOS
```bash
pip3 install numpy matplotlib seaborn pandas

# Veya virtual environment ile
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### BaÄŸÄ±mlÄ±lÄ±k SÃ¼rÃ¼m Bilgileri

| Paket | Minimum SÃ¼rÃ¼m | AÃ§Ä±klama |
|--------|---------------|----------|
| numpy | >=1.20.0 | SayÄ±sal iÅŸlemler, Q tablosu, vektÃ¶rizasyon |
| matplotlib | >=3.3.0 | Grafik Ã§izimi, PNG Ã§Ä±ktÄ± |
| seaborn | >=0.11.0 | Modern gÃ¶rselleÅŸtirmeler, heatmap, lineplot |
| pandas | >=1.3.0 | DataFrame, veri manipÃ¼lasyonu |

## ğŸ’» KullanÄ±m

### VarsayÄ±lan Ayarlarla Ã‡alÄ±ÅŸtÄ±rma

```bash
python tictactoe_rl.py
```

Bu komut:
- Q-Learning ve SARSA'yÄ± self-play modunda eÄŸitir
- Cross-play karÅŸÄ±laÅŸmalarÄ± yapar
- Random rakibe karÅŸÄ± performans Ã¶lÃ§er
- Minimax ve SARSA'ya karÅŸÄ± turnuva yapar
- TÃ¼m sonuÃ§larÄ± `outputs/` klasÃ¶rÃ¼ne kaydeder

### HÄ±zlÄ± Test (Daha Az BÃ¶lÃ¼m)

```bash
python tictactoe_rl.py --self-play-episodes 1000 --cross-play-episodes 1000 \
  --baseline-episodes 500 --tournament-games 200 --plot
```

### Tam Deney (Daha Fazla BÃ¶lÃ¼m)

```bash
python tictactoe_rl.py --self-play-episodes 10000 --cross-play-episodes 10000 \
  --baseline-episodes 5000 --tournament-games 1000
```

### GÃ¶rselleÅŸtirmeyi Kapatmak

```bash
python tictactoe_rl.py --no-plot
```

### Ã–zel Hiperparametreler

```bash
python tictactoe_rl.py --alpha 0.05 --gamma 1.0 \
  --epsilon-start 0.9 --epsilon-end 0.001
```

## ğŸ“ Proje YapÄ±sÄ±

```
kod2/
â”œâ”€â”€ tictactoe_rl.py    # Ana Python dosyasÄ± (~1200 satÄ±r)
â”œâ”€â”€ report.md          # DetaylÄ± proje raporu
â”œâ”€â”€ README.md          # Bu dosya
â”œâ”€â”€ requirements.txt   # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ .gitignore        # Git hariÃ§ tutma kurallarÄ±
â””â”€â”€ outputs/          # Ã‡Ä±ktÄ± klasÃ¶rÃ¼ (Ã§alÄ±ÅŸtÄ±ktan sonra)
    â”œâ”€â”€ results.json       # TÃ¼m deney sonuÃ§larÄ±
    â”œâ”€â”€ tournament.csv    # Turnuva karÅŸÄ±laÅŸtÄ±rmalarÄ±
    â”œâ”€â”€ training.png      # EÄŸitim trendi
    â”œâ”€â”€ tournament.png    # Turnuva sonuÃ§larÄ±
    â”œâ”€â”€ heatmap_q.png     # Q-Learning hÃ¼cre tercihleri
    â””â”€â”€ heatmap_sarsa.png # SARSA hÃ¼cre tercihleri
```

## ğŸ§  Algoritmalar

### Q-Learning (Off-Policy)

Q-Learning, off-policy bir algoritmadÄ±r. Hedef politikayÄ± greedy (keÅŸifsiz) kabul eder,
ancak Ã¶ÄŸrenme sÄ±rasÄ±nda epsilon-greedy (keÅŸifli) davranÄ±ÅŸ politikasÄ± kullanÄ±r.

```
Q(s,a) â† Q(s,a) + Î± [r + Î³ * max_a' Q(s',a') - Q(s,a)]
```

**Ã–zellikler:**
- Agresif Ã¶ÄŸrenme (max operatÃ¶rÃ¼)
- HÄ±zlÄ± yakÄ±nsama potansiyeli
- Daha kararsÄ±z olabilir
- Teorik olarak optimal hedef politika

### SARSA (On-Policy)

SARSA, on-policy bir algoritmadÄ±r. DavranÄ±ÅŸ politikasÄ± ile hedef politika aynÄ±dÄ±r.

```
Q(s,a) â† Q(s,a) + Î± [r + Î³ * Q(s',a') - Q(s,a)]
```

**Ã–zellikler:**
- Stabil ve temkinli Ã¶ÄŸrenme
- Epsilon-greedy ile tutarlÄ±
- Genellikle daha iyi gerÃ§ek performans
- KeÅŸif ve sÃ¶mÃ¼rÃ¼ dengesi

### KarÅŸÄ±laÅŸtÄ±rma

| Ã–zellik | Q-Learning | SARSA |
|----------|------------|--------|
| Politika TÃ¼rÃ¼ | Off-policy | On-policy |
| GÃ¼ncelleme | max Q(s',a') | Q(s',a') |
| KeÅŸif Stratejisi | Agresif | Dengeli |
| Stabilite | KararsÄ±z | Stabil |
| YakÄ±nsama HÄ±zÄ± | HÄ±zlÄ± | Orta |
| Optimalite | Teorik optimal | Neredeyse optimal |

## ğŸ“Š Ã‡Ä±ktÄ±lar

### results.json

```json
{
  "config": { ... },
  "training": {
    "Q self-play": { "wins": 1552, "draws": 2987, ... },
    "SARSA self-play": { ... },
    ...
  },
  "tournament": {
    "Q vs Random": { "win_rate": 0.728, ... },
    ...
  },
  "q_variance": {
    "Q-X": 0.00268,
    "Q-O": 0.00085,
    ...
  }
}
```

### tournament.csv

| matchup | wins | draws | losses | win_rate | draw_rate | loss_rate |
|---------|-------|--------|---------|----------|-----------|-----------|
| Q vs Random | 364 | 40 | 96 | 0.728 | 0.080 | 0.192 |
| SARSA vs Random | 361 | 51 | 88 | 0.722 | 0.102 | 0.176 |
| ... | ... | ... | ... | ... | ... | ... |

### GÃ¶rselleÅŸtirmeler

- **training.png**: EÄŸitim sÃ¼recinde hareketli ortalama kazanma oranÄ± trendi
  - Seaborn lineplot
  - Her eÄŸitim aÅŸamasÄ± iÃ§in ayrÄ± Ã§izgi
  - 300 DPI yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼k

- **tournament.png**: Turnuva sonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±rmasÄ±
  - Seaborn stacked bar chart
  - Kazanma/Beraberlik/MaÄŸlubiyet oranlarÄ±
  - `deep` color palette

- **heatmap_q.png**: Q-Learning hÃ¼cre tercih yoÄŸunluÄŸu
  - Seaborn heatmap
  - `flare` colormap (daha koyu renkler)
  - 3x3 grid, beyaz anotasyonlar

- **heatmap_sarsa.png**: SARSA hÃ¼cre tercih yoÄŸunluÄŸu
  - AynÄ± format, farklÄ± veriler

## ğŸ“ˆ SonuÃ§lar

### Rastgele Rakibe KarÅŸÄ±

| Algoritma | Kazanma | Beraberlik | MaÄŸlubiyet |
|-----------|----------|------------|------------|
| Q-Learning | ~97% | ~2% | ~1% |
| SARSA | ~88% | ~9% | ~3% |

### Minimax Rakibe KarÅŸÄ±

| Algoritma | Kazanma | Beraberlik | MaÄŸlubiyet |
|-----------|----------|------------|------------|
| Q-Learning | 0% | ~53% | ~47% |
| SARSA | 0% | ~55% | ~45% |

### SonuÃ§ Analizi

1. **Random PerformansÄ±**: Her iki algoritma random rakibi kolayca yener
   - Q-Learning daha agresif olduÄŸu iÃ§in daha yÃ¼ksek kazanma oranÄ±
   - SARSA daha temkinli, ama yeterince gÃ¼Ã§lÃ¼

2. **Minimax PerformansÄ±**: Ä°ki algoritma da Minimax'a karÅŸÄ± kazanamaz
   - Optimal strateji olduÄŸu iÃ§in kazanma imkansÄ±z
   - YÃ¼ksek beraberlik oranÄ± baÅŸarÄ±lÄ± Ã¶ÄŸrenme gÃ¶steriyor

3. **Self-Play Sonucu**: Beraberlik oranÄ± artar
   - Ä°ki gÃ¼Ã§lÃ¼ ajan karÅŸÄ±laÅŸtÄ±ÄŸÄ±nda beraberlik yaygÄ±n

Daha fazla detay iÃ§in [`report.md`](report.md) dosyasÄ±na bakÄ±n.

## âš™ï¸ CLI SeÃ§enekleri

| Parametre | VarsayÄ±lan | Tip | AÃ§Ä±klama |
|-----------|-----------|------|----------|
| `--alpha` | 0.1 | float | Ã–ÄŸrenme oranÄ± (0 < Î± â‰¤ 1) |
| `--gamma` | 0.95 | float | Ä°skonto faktÃ¶rÃ¼ (0 â‰¤ Î³ â‰¤ 1) |
| `--epsilon-start` | 1.0 | float | BaÅŸlangÄ±Ã§ keÅŸif oranÄ± |
| `--epsilon-end` | 0.01 | float | BitiÅŸ keÅŸif oranÄ± |
| `--epsilon-decay` | 0.995 | float | Her bÃ¶lÃ¼mde epsilon dÃ¼ÅŸÃ¼rme oranÄ± |
| `--self-play-episodes` | 5000 | int | Self-play bÃ¶lÃ¼m sayÄ±sÄ± |
| `--cross-play-episodes` | 5000 | int | Cross-play bÃ¶lÃ¼m sayÄ±sÄ± |
| `--baseline-episodes` | 3000 | int | Baseline bÃ¶lÃ¼m sayÄ±sÄ± |
| `--tournament-games` | 500 | int | Turnuva oyun sayÄ±sÄ± |
| `--moving-avg-window` | 200 | int | Hareketli ortalama penceresi |
| `--log-interval` | 500 | int | Log aralÄ±ÄŸÄ± (0 = kapalÄ±) |
| `--convergence-threshold` | 0.8 | float | YakÄ±nsama eÅŸiÄŸi (0-1) |
| `--seed` | 42 | int | Rastgelelik tohumu |
| `--output-dir` | outputs | str | Ã‡Ä±ktÄ± klasÃ¶rÃ¼ |
| `--plot` | True | flag | Grafikleri Ã¼ret (varsayÄ±lan) |
| `--no-plot` | False | flag | Grafikleri kapat |

### Test Etme

```bash
# HÄ±zlÄ± test
python tictactoe_rl.py --self-play-episodes 100 --plot

# Tam test
python tictactoe_rl.py
```


## ğŸ“š Kaynaklar

- [Sutton & Barto: Reinforcement Learning](http://incompleteideas.net/book/RLbook2020.pdf)
- [Seaborn Documentation](https://seaborn.pydata.org/)
- [Tic-Tac-Toe MDP](https://en.wikipedia.org/wiki/Tic-tac-toe)

## ğŸ“„ Lisans

Bu proje **MIT LisansÄ±** altÄ±nda lisanslanmÄ±ÅŸtÄ±r.

### MIT LisansÄ± Ã–zeti

- âœ… Ticari kullanÄ±m izni
- âœ… Modifikasyon izni
- âœ… DaÄŸÄ±tÄ±m izni
- âœ… KiÅŸisel kullanÄ±m izni
- âš ï¸ Lisans ve telif hakkÄ± bildirimi zorunludur
- âŒ Garanti verilmemiÅŸtir (AS IS)

### Tam Metin

LisansÄ±n tam metni iÃ§in [`LICENSE`](LICENSE) dosyasÄ±na bakÄ±n.

### AlÄ±ntÄ± ve AtÄ±f

Bu projeyi kullandÄ±ÄŸÄ±nÄ±zda lÃ¼tfen aÅŸaÄŸÄ±daki alÄ±ntÄ±yÄ± kullanÄ±n:

```bibtex
@software{tictactoe_rl_2025,
  title = {Tic-Tac-Toe Q-Learning vs SARSA},
  author = {Heimdilon},
  year = {2025},
  url = {https://github.com/heimdilon/tictactoe-rl-q-vs-sarsa}
}
```

Veya basit atÄ±f:

> Heimdilon. (2025). Tic-Tac-Toe Q-Learning vs SARSA. GitHub. https://github.com/heimdilon/tictactoe-rl-q-vs-sarsa

---

**Yazar**: [Heimdilon](https://github.com/heimdilon)

â­ï¸ Bu repo'yu beÄŸendiyseniz star vermeyi unutmayÄ±n!
